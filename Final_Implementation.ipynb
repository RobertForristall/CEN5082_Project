{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceaf3a41-33bf-4d93-84fc-4db9dfd846cc",
   "metadata": {},
   "source": [
    "# Final Pipeline\n",
    "## Author: Robert Forristall\n",
    "\n",
    "This document implements the final complete implementation of my work for FIU Spring 2024 CEN 5082 Project: Using Neuromancer tool to integrate physical constraints for an HPC Application (Power Grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "59ec5212-8bdf-4e19-bb0e-a9d45b6185d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Generation\n",
    "import os\n",
    "import re\n",
    "import random as rand\n",
    "import math\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset Preparation\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import string\n",
    "import numpy as np\n",
    "from neuromancer.dataset import DictDataset\n",
    "\n",
    "# Model Generation\n",
    "from neuromancer.system import Node\n",
    "from neuromancer.loss import PenaltyLoss\n",
    "from neuromancer.trainer import Trainer\n",
    "from neuromancer.problem import Problem\n",
    "from neuromancer.constraint import variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd81fae-3e3c-4eb7-8b2b-a4beb5fed56f",
   "metadata": {},
   "source": [
    "## Case Generation\n",
    "\n",
    "This section of code declares all of the functions that are used for the generation of new cases that can be used to train a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "54319458-6491-4ba7-9a40-1bc5733fb94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleLowBound(pd, t):\n",
    "    return (1-t)*pd;\n",
    "\n",
    "def sampleHighBound(pd, t):\n",
    "    return (1+t)*pd;\n",
    "\n",
    "def sampleLoadValue(pd, t):\n",
    "    if (pd != 0):\n",
    "        low = sampleLowBound(pd, t)*100\n",
    "        high = sampleHighBound(pd, t)*100\n",
    "        if (low < high):\n",
    "            return rand.randrange(math.floor(low), math.ceil(high))/100\n",
    "        else:\n",
    "            return rand.randrange(math.floor(high), math.ceil(low))/100\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_function_name(fileStr, index):\n",
    "    return re.findall(\"function .* = .*\", fileStr)[0] + f\"_{index}\"\n",
    "\n",
    "def get_mpc_version(fileStr):\n",
    "    return re.findall(\"mpc.version = '\\d';\", fileStr)[0]\n",
    "\n",
    "def get_mpc_base(fileStr):\n",
    "    return re.findall(\"mpc.baseMVA = \\d*;\", fileStr)[0]\n",
    "\n",
    "def get_bus_data(fileStr, delta):\n",
    "    pattern = r\"mpc.bus = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    bus_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    return_str = \"\"\n",
    "    return_str += bus_data[0] + \"\\n\"\n",
    "    for i in range(1, len(bus_data)-1):\n",
    "        current_row = bus_data[i].split(\"\\t\")\n",
    "        current_row[3] = str(sampleLoadValue(float(current_row[3]), delta))\n",
    "        current_row[4] = str(sampleLoadValue(float(current_row[4]), delta))\n",
    "        return_str += \"\\t\".join(current_row) + \"\\n\"\n",
    "    return_str += bus_data[-1] + \"\\n\"\n",
    "    return return_str\n",
    "\n",
    "def get_gen_data(fileStr):\n",
    "    pattern = r\"mpc.gen = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    gen_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    return_str = \"\"\n",
    "    return_str += gen_data[0] + \"\\n\"\n",
    "    for i in range(1, len(gen_data)-1):\n",
    "        return_str += gen_data[i] + \"\\n\"\n",
    "    return_str += gen_data[-1] + \"\\n\"\n",
    "    return return_str\n",
    "\n",
    "def get_branch_data(fileStr):\n",
    "    pattern = r\"mpc.branch = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    branch_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    return_str = \"\"\n",
    "    return_str += branch_data[0] + \"\\n\"\n",
    "    for i in range(1, len(branch_data)-1):\n",
    "        return_str += branch_data[i] + \"\\n\"\n",
    "    return_str += branch_data[-1] + \"\\n\"\n",
    "    return return_str\n",
    "\n",
    "def get_gencost_data(fileStr):\n",
    "    pattern = r\"mpc.gencost = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    gencost_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    return_str = \"\"\n",
    "    return_str += gencost_data[0] + \"\\n\"\n",
    "    for i in range(1, len(gencost_data)-1):\n",
    "        return_str += gencost_data[i] + \"\\n\"\n",
    "    return_str += gencost_data[-1] + \"\\n\"\n",
    "    return return_str\n",
    "\n",
    "def get_full_case(fileStr, index, delta):\n",
    "    case_str = \"\\n\\n\".join([\n",
    "        get_function_name(fileStr, index),\n",
    "        get_mpc_version(fileStr),\n",
    "        get_mpc_base(fileStr),\n",
    "        get_bus_data(fileStr, delta),\n",
    "        get_gen_data(fileStr),\n",
    "        get_branch_data(fileStr),\n",
    "        get_gencost_data(fileStr)\n",
    "    ])\n",
    "    return case_str\n",
    "\n",
    "def get_case_as_string(case_file):\n",
    "    with open(case_file, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "def create_test_cases(fileStr, total_number_of_cases, delta):\n",
    "    if not os.path.exists(\"generated_cases\"):\n",
    "        print(\"Generating New Cases...\")\n",
    "        os.mkdir(\"generated_cases\")\n",
    "        for i in tqdm(range(total_number_of_cases)):\n",
    "            with open(f\"generated_cases/GeneratedCase{i}.m\", \"w\") as file:\n",
    "                file.write(get_full_case(fileStr, i, delta))\n",
    "    else:\n",
    "        print(\"Cases already generated, skipping...\")\n",
    "        # shutil.rmtree(\"generated_cases\")\n",
    "        # os.mkdir(\"generated_cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b027667-c91e-49cf-a541-e7cb104856c8",
   "metadata": {},
   "source": [
    "## Run generated cases through MIPS\n",
    "\n",
    "### In future iterations this stage will be automated by implementing MIPS in python\n",
    "\n",
    "Run the generated files through MIPS and save the following for each case in its own excel within its own directory inside 'mips_results/' + the variable name lowercase:\n",
    "- Optimization vector X (Va, Vm, Pg, Qg), saved in 'mips_results/x'\n",
    "- Langrangian equality metric Lambda, saved in 'mips_results/lambda'\n",
    "- Slack variable vector Z, saved in 'mips_results/z'\n",
    "- Langrangian inequality metric Mu, saved in 'mips_results/mu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de63ab71-de70-495e-b499-ce629f8671c6",
   "metadata": {},
   "source": [
    "## Prepare Train/Test Data (MTL Solo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "0e0c1b48-ddcc-4bfa-aa90-d0cce5d86902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bus_count(fileStr):\n",
    "    pattern = r\"mpc.bus = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    bus_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    return len(bus_data)-2\n",
    "\n",
    "def get_gen_count(fileStr):\n",
    "    pattern = r\"mpc.gen = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    gen_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    return len(gen_data)-2\n",
    "\n",
    "def get_pds_from_case(file):\n",
    "    fileStr = get_case_as_string(file)\n",
    "    pattern = r\"mpc.bus = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    bus_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    pds = []\n",
    "    for i in range(1, len(bus_data)-1):\n",
    "        current_row = bus_data[i].split(\"\\t\")\n",
    "        pds.append(current_row[3])\n",
    "    return pds\n",
    "\n",
    "def get_qds_from_case(file):\n",
    "    fileStr = get_case_as_string(file)\n",
    "    pattern = r\"mpc.bus = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    bus_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    qds = []\n",
    "    for i in range(1, len(bus_data)-1):\n",
    "        current_row = bus_data[i].split(\"\\t\")\n",
    "        qds.append(current_row[4])\n",
    "    return qds\n",
    "\n",
    "def build_pds_csv(bus_size):\n",
    "    print(\"Building pds...\")\n",
    "    pd_data = pd.DataFrame(columns = [f\"bus_{x+1}\" for x in range(bus_size)]);\n",
    "    for file in tqdm(os.listdir(\"generated_cases\")):\n",
    "        pd_data.loc[len(pd_data.index)] = get_pds_from_case(\"generated_cases/\"+file)\n",
    "    pd_data.to_csv(\"bus_Pds.csv\", index=False)\n",
    "\n",
    "def build_qds_csv(bus_size):\n",
    "    print(\"Building qds...\")\n",
    "    qd_data = pd.DataFrame(columns = [f\"bus_{x+1}\" for x in range(bus_size)]);\n",
    "    for file in tqdm(os.listdir(\"generated_cases\")):\n",
    "        qd_data.loc[len(qd_data.index)] = get_qds_from_case(\"generated_cases/\"+file)\n",
    "    qd_data.to_csv(\"bus_Qds.csv\", index=False)\n",
    "\n",
    "def load_data_MTL(bus_dir, bus_size, gen_size):\n",
    "    # Load Bus Input Data\n",
    "    if not os.path.exists(\"bus_Pds.csv\"):\n",
    "        build_pds_csv(bus_size)\n",
    "    if not os.path.exists(\"bus_Qds.csv\"):\n",
    "        build_qds_csv(bus_size)\n",
    "    Pds = pd.read_csv(bus_dir + 'bus_Pds.csv').to_numpy()\n",
    "    Qds = pd.read_csv(bus_dir + 'bus_Qds.csv').to_numpy()\n",
    "    input_data = []\n",
    "    \n",
    "    # Load Bus Label Data\n",
    "    Va = []\n",
    "    Vm = []\n",
    "    Pg = []\n",
    "    Qg = []\n",
    "    Lambda = []\n",
    "    Z = []\n",
    "    Mu = []\n",
    "    for file in tqdm(os.listdir(bus_dir + 'mips_results/x')):\n",
    "        case_id = int(file.strip(string.ascii_letters + \".\"))\n",
    "        input_data.append(np.concatenate((Pds[case_id], Qds[case_id]), axis=0))\n",
    "        x_data = pd.read_csv(bus_dir + 'mips_results/x/' + file, header=None).to_numpy().flatten()\n",
    "        Va.append(x_data[0:bus_size])\n",
    "        Vm.append(x_data[bus_size:bus_size*2])\n",
    "        Pg.append(x_data[bus_size*2:(bus_size*2) + gen_size])\n",
    "        Qg.append(x_data[(bus_size*2) + gen_size:(bus_size*2) + (gen_size*2)])\n",
    "        # Lambda.append(json.load(open('mips_results/lambda/myCase' + str(case_id) + '.json'))[\"eqnonlin\"])\n",
    "        Lambda.append(pd.read_csv(bus_dir + 'mips_results/lambda/' + file, header=None).to_numpy().flatten())\n",
    "        Z.append(pd.read_csv(bus_dir + 'mips_results/z/' + file, header=None).to_numpy().flatten())\n",
    "        Mu.append(pd.read_csv(bus_dir + 'mips_results/mu/' + file, header=None).to_numpy().flatten())\n",
    "    return (input_data, Va, Vm, Pg, Qg, Lambda, Z, Mu)\n",
    "\n",
    "# Craate Dataset\n",
    "class CustomOpfMultiTaskDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_data, Va, Vm, Pg, Qg, Lambda, Z, Mu):\n",
    "        self.x_data = np.array(x_data).astype(np.float32)\n",
    "        self.Va = np.array(Va).astype(np.float32)\n",
    "        self.Vm = np.array(Vm).astype(np.float32)\n",
    "        self.Pg = np.array(Pg).astype(np.float32)\n",
    "        self.Qg = np.array(Qg).astype(np.float32)\n",
    "        self.Lambda = np.array(Lambda).astype(np.float32)\n",
    "        self.Z = np.array(Z).astype(np.float32)\n",
    "        self.Mu = np.array(Mu).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'x_val': self.x_data[index],\n",
    "            'Va': self.Va[index],\n",
    "            'Vm': self.Vm[index],\n",
    "            'Pg': self.Pg[index],\n",
    "            'Qg': self.Qg[index],\n",
    "            'Lambda': self.Lambda[index],\n",
    "            'Z': self.Z[index],\n",
    "            'Mu': self.Mu[index]\n",
    "        }\n",
    "\n",
    "def create_data_loaders(split_index, batch_size, data_tuple, bus_size):\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        CustomOpfMultiTaskDataset(\n",
    "            data_tuple[0][0:split_index],\n",
    "            data_tuple[1][0:split_index],\n",
    "            data_tuple[2][0:split_index],\n",
    "            data_tuple[3][0:split_index],\n",
    "            data_tuple[4][0:split_index],\n",
    "            data_tuple[5][0:split_index],\n",
    "            data_tuple[6][0:split_index],\n",
    "            data_tuple[7][0:split_index],\n",
    "        ), shuffle=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    validate_dataloader = DataLoader(\n",
    "        CustomOpfMultiTaskDataset(\n",
    "            data_tuple[0][split_index:],\n",
    "            data_tuple[1][split_index:],\n",
    "            data_tuple[2][split_index:],\n",
    "            data_tuple[3][split_index:],\n",
    "            data_tuple[4][split_index:],\n",
    "            data_tuple[5][split_index:],\n",
    "            data_tuple[6][split_index:],\n",
    "            data_tuple[7][split_index:],\n",
    "        ), shuffle=False, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return (train_dataloader, validate_dataloader)\n",
    "\n",
    "def get_MTL_solo_data(bus_size, gen_size):\n",
    "    if (not os.path.exists(f\"trainData.pth\") or not os.path.exists(f\"validData.pth\")):\n",
    "        print(\"Data Loaders for MTL solo not found, loading from raw data\\n\")\n",
    "        (train_dataloader, validate_dataloader) = create_data_loaders(8000, 32, load_data_MTL(\"\", bus_size, gen_size), bus_size)\n",
    "        torch.save(train_dataloader, \"trainData.pth\")\n",
    "        torch.save(validate_dataloader, \"validData.pth\")\n",
    "    else:\n",
    "        print(\"Data Loaders for MTL solo found, loading from .pth files\\n\")\n",
    "        train_dataloader = torch.load(\"trainData.pth\")\n",
    "        validate_dataloader = torch.load(\"validData.pth\")\n",
    "\n",
    "    return train_dataloader, validate_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f279fa-c209-4ad1-b4de-a2d20c32d7bc",
   "metadata": {},
   "source": [
    "## Prepare Train/Test Data (MTL With Neuromancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "81a7f161-fc2a-467d-b93e-f8a8049f7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_dict(Pds, Qds, Va, Vm, Pg, Qg, Lambda, Z, Mu, start_index, end_index):\n",
    "        return {\n",
    "        \"Pd\": torch.tensor(Pds[start_index:end_index], dtype=torch.float32),\n",
    "        \"Qd\": torch.tensor(Qds[start_index:end_index], dtype=torch.float32),\n",
    "        \"Va\": torch.tensor(Va[start_index:end_index], dtype=torch.float32),\n",
    "        \"Vm\": torch.tensor(Vm[start_index:end_index], dtype=torch.float32),\n",
    "        \"Pg\": torch.tensor(Pg[start_index:end_index], dtype=torch.float32),\n",
    "        \"Qg\": torch.tensor(Qg[start_index:end_index], dtype=torch.float32),\n",
    "        \"Lambda\": torch.tensor(Lambda[start_index:end_index], dtype=torch.float32),\n",
    "        \"Z\": torch.tensor(Z[start_index:end_index], dtype=torch.float32),\n",
    "        \"Mu\": torch.tensor(Mu[start_index:end_index], dtype=torch.float32)\n",
    "    }\n",
    "\n",
    "def load_data():\n",
    "    train_data = torch.load(\"neuromancerTrainData.pth\")\n",
    "    dev_data = torch.load(\"neuromancerDevData.pth\")\n",
    "    test_data = torch.load(\"neuromancerTestData.pth\")\n",
    "    out_size = pd.read_csv(\"output_size.csv\", header=None).to_numpy()[0][0]\n",
    "\n",
    "    return train_data, dev_data, test_data, out_size\n",
    "    \n",
    "\n",
    "def create_neuromancer_data(bus_dir, bus_size, gen_size, train_percent, dev_percent, test_percent):\n",
    "    # Load Bus Input Data\n",
    "    if not os.path.exists(\"bus_Pds.csv\"):\n",
    "        build_pds_csv(bus_size)\n",
    "    if not os.path.exists(\"bus_Qds.csv\"):\n",
    "        build_qds_csv(bus_size)\n",
    "    Pds = pd.read_csv(bus_dir + 'bus_Pds.csv').to_numpy()\n",
    "    Qds = pd.read_csv(bus_dir + 'bus_Qds.csv').to_numpy()\n",
    "    input_data = []\n",
    "\n",
    "    num_of_cases = Pds.shape[0]\n",
    "    train_end_index = int(num_of_cases*train_percent)\n",
    "    dev_end_index = int(train_end_index + (num_of_cases*dev_percent))\n",
    "    test_end_index = num_of_cases\n",
    "    print(train_end_index)\n",
    "    print(dev_end_index)\n",
    "\n",
    "    out_size = 0\n",
    "    \n",
    "    #Load Bus Label Data\n",
    "    Va = []\n",
    "    Vm = []\n",
    "    Pg = []\n",
    "    Qg = []\n",
    "    Lambda = []\n",
    "    Z = []\n",
    "    Mu = []\n",
    "    for file in tqdm(os.listdir(bus_dir + 'mips_results/x')):\n",
    "        case_id = int(file.strip(string.ascii_letters + \".\"))\n",
    "        # input_data.append(np.concatenate((Pds[case_id], Qds[case_id]), axis=0))\n",
    "        x_data = pd.read_csv(bus_dir + 'mips_results/x/' + file, header=None).to_numpy().flatten()\n",
    "        Va.append(x_data[0:bus_size])\n",
    "        Vm.append(x_data[bus_size:bus_size*2])\n",
    "        Pg.append(x_data[bus_size*2:(bus_size*2) + gen_size])\n",
    "        Qg.append(x_data[(bus_size*2) + gen_size:(bus_size*2) + (gen_size*2)])\n",
    "        # Lambda.append(json.load(open('mips_results/lambda/myCase' + str(case_id) + '.json'))[\"eqnonlin\"])\n",
    "        lambda_data = pd.read_csv(bus_dir + 'mips_results/lambda/' + file, header=None).to_numpy().flatten()\n",
    "        Lambda.append(lambda_data)\n",
    "        z_data = pd.read_csv(bus_dir + 'mips_results/z/' + file, header=None).to_numpy().flatten()\n",
    "        Z.append(z_data)\n",
    "        mu_data = pd.read_csv(bus_dir + 'mips_results/mu/' + file, header=None).to_numpy().flatten()\n",
    "        Mu.append(mu_data)\n",
    "        if out_size == 0:\n",
    "            out_size = x_data.shape[0] + lambda_data.shape[0] + z_data.shape[0] + mu_data.shape[0]\n",
    "    # return (input_data, Va, Vm, Pg, Qg, Lambda, Z, Mu)\n",
    "\n",
    "    train_dict = convert_data_to_dict(Pds, Qds, Va, Vm, Pg, Qg, Lambda, Z, Mu, 0, train_end_index)\n",
    "    dev_dict = convert_data_to_dict(Pds, Qds, Va, Vm, Pg, Qg, Lambda, Z, Mu, train_end_index, dev_end_index)\n",
    "    test_dict = convert_data_to_dict(Pds, Qds, Va, Vm, Pg, Qg, Lambda, Z, Mu, dev_end_index, test_end_index)\n",
    "    \n",
    "    train_dict = DictDataset(train_dict, name='train')\n",
    "    dev_dict = DictDataset(dev_dict, name='dev')\n",
    "    test_dict = DictDataset(test_dict, name='test')\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dict, batch_size=32, num_workers=0,\n",
    "                                               collate_fn=train_dict.collate_fn, shuffle=True)\n",
    "    dev_loader = torch.utils.data.DataLoader(dev_dict, batch_size=32, num_workers=0,\n",
    "                                             collate_fn=dev_dict.collate_fn, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dict, batch_size=32, num_workers=0,\n",
    "                                             collate_fn=test_dict.collate_fn, shuffle=True)\n",
    "    \n",
    "    return train_loader, dev_loader, test_loader, out_size\n",
    "\n",
    "def get_neuromancer_data(bus_size, gen_size):\n",
    "    bus_dir = f\"generated_cases\"\n",
    "    train_loader = None\n",
    "    dev_loader = None\n",
    "    test_loader = None\n",
    "    if (not os.path.exists(\"neuromancerTrainData.pth\")):\n",
    "        print(\"Data Loaders for MTL in Neuromancer not found, loading from raw data\\n\")\n",
    "        train_data, dev_data, test_data, out_size = create_neuromancer_data(f\"\", bus_size, gen_size, 0.7, 0.15, 0.15)\n",
    "        torch.save(train_data, \"neuromancerTrainData.pth\")\n",
    "        torch.save(dev_data, \"neuromancerDevData.pth\")\n",
    "        torch.save(test_data, \"neuromancerTestData.pth\")\n",
    "        df = pd.DataFrame(columns=['output_size'])\n",
    "        df.loc[len(df.index)] = [out_size]\n",
    "        df.to_csv(\"output_size.csv\", header=False, index=False)\n",
    "    else:\n",
    "        print(\"Data Loaders for MTL in Neuromancer found, loading from .pth files\\n\")\n",
    "        train_data, dev_data, test_data, out_size = load_data()\n",
    "\n",
    "    return train_data, dev_data, test_data, out_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f509620-5657-45e0-9466-c1f600f8378e",
   "metadata": {},
   "source": [
    "## Model Generation Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "9353c840-d6c8-4e7c-9cfa-19c81357420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z_mu_size():\n",
    "    return pd.read_csv(\"mips_results/z/\"+os.listdir(\"mips_results/z\")[0], header=None).to_numpy().shape[0]\n",
    "\n",
    "def generate_layers(bus_size, gen_size, z_mu_size):\n",
    "        # print(f\"\\nBus Size: {bus_size}, Gen Size: {gen_size}\")\n",
    "        layer_1_sizes = [x for x in range(bus_size*2, (bus_size*2) + ((int(bus_size/4))*5), int(bus_size/4))]\n",
    "        layer_x_sizes_1 = [x for x in range((bus_size*2) + ((int(bus_size/4))*5), bus_size, -int(bus_size/4))]\n",
    "        layer_x_sizes_1[-1] = bus_size\n",
    "        layer_x_sizes_1[0] = layer_1_sizes[-1]\n",
    "        layer_x_sizes_2 = [x for x in range((bus_size*2) + ((int(bus_size/4))*5), gen_size, -int(bus_size/4))]\n",
    "        layer_x_sizes_2[-1] = gen_size\n",
    "        layer_x_sizes_2[0] = layer_1_sizes[-1]\n",
    "        layer_l_sizes = [x for x in range((bus_size*2) + ((int(bus_size/4))*5), (bus_size*2)+1, -int(bus_size/4))]\n",
    "        layer_l_sizes[-1] = (bus_size*2)+1\n",
    "        layer_l_sizes[0] = layer_1_sizes[-1]\n",
    "        layer_z_sizes = [x for x in range((bus_size*2)+(gen_size*2), ((bus_size*2)+(gen_size*2))*4, int(bus_size/2))]+ [x for x in range(((bus_size*2)+(gen_size*2))*4, z_mu_size, -int(bus_size/2))]\n",
    "        layer_z_sizes[-1] = z_mu_size\n",
    "        layer_mu_sizes = [x for x in range(z_mu_size, ((bus_size*2)+(gen_size*2))*4, int(bus_size/2))]+ [x for x in range(((bus_size*2)+(gen_size*2))*4, z_mu_size, -int(bus_size/2))]\n",
    "        layer_mu_sizes[-1] = z_mu_size\n",
    "        return layer_1_sizes, layer_x_sizes_1, layer_x_sizes_2, layer_l_sizes, layer_z_sizes, layer_mu_sizes\n",
    "\n",
    "def get_device():\n",
    "    # print(torch.__version__)\n",
    "    # print(torch.version.cuda)\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    # print(f\"Using {device} device\")\n",
    "    # if device == \"cuda\":\n",
    "        # print(torch.cuda.get_device_name(0))\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2e453-959f-4e63-9a4d-f5ca078fac07",
   "metadata": {},
   "source": [
    "## Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "cb6b9f1f-8675-4571-8bd1-edf990281430",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMultiTaskNetSolo(torch.nn.Module):\n",
    "    def __init__(self, bus_size, gen_size, z_mu_size):\n",
    "        super(TestMultiTaskNetSolo, self).__init__()\n",
    "        # Sahred layers (Input: 28, Output: 58)\n",
    "\n",
    "        layer_1_sizes, layer_x_sizes_1, layer_x_sizes_2, layer_l_sizes, layer_z_sizes, layer_mu_sizes = generate_layers(bus_size, gen_size, z_mu_size)\n",
    "\n",
    "        # print(layer_1_sizes)\n",
    "        # print(layer_x_sizes_1)\n",
    "        # print(layer_x_sizes_2)\n",
    "        # print(layer_l_sizes)\n",
    "        # print(layer_z_sizes)\n",
    "        # print(layer_mu_sizes)\n",
    "        \n",
    "        shared_modules = []\n",
    "        x_bus_modules_1 = []\n",
    "        x_bus_modules_2 = []\n",
    "        x_gen_modules_1 = []\n",
    "        x_gen_modules_2 = []\n",
    "        l_modules = []\n",
    "        z_modules = []\n",
    "        mu_modules = []\n",
    "\n",
    "        for i in range(1, len(layer_1_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                shared_modules.append(torch.nn.Linear(layer_1_sizes[i-1], layer_1_sizes[i]))\n",
    "                # shared_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                shared_modules.append(torch.nn.Linear(layer_1_sizes[i-1], layer_1_sizes[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_1)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_bus_modules_1.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "                # x_bus_modules_1.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_bus_modules_1.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_1)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_bus_modules_2.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "                # x_bus_modules_2.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_bus_modules_2.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_2)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_gen_modules_1.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "                # x_gen_modules_1.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_gen_modules_1.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_2)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_gen_modules_2.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "                # x_gen_modules_2.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_gen_modules_2.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "\n",
    "        for i in range(1, len(layer_l_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                l_modules.append(torch.nn.Linear(layer_l_sizes[i-1], layer_l_sizes[i]))\n",
    "                # l_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                l_modules.append(torch.nn.Linear(layer_l_sizes[i-1], layer_l_sizes[i]))\n",
    "\n",
    "        for i in range(1, len(layer_z_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                z_modules.append(torch.nn.Linear(layer_z_sizes[i-1], layer_z_sizes[i]))\n",
    "                # z_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                z_modules.append(torch.nn.Linear(layer_z_sizes[i-1], layer_z_sizes[i]))\n",
    "\n",
    "        for i in range(1, len(layer_mu_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                mu_modules.append(torch.nn.Linear(layer_mu_sizes[i-1], layer_mu_sizes[i]))\n",
    "                # mu_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                mu_modules.append(torch.nn.Linear(layer_mu_sizes[i-1], layer_mu_sizes[i]))\n",
    "        \n",
    "        self.net = torch.nn.Sequential(*shared_modules)\n",
    "        \n",
    "        self.n_features = bus_size*2\n",
    "        self.net.fc = torch.nn.Identity()\n",
    "\n",
    "        self.x_l_heads = torch.nn.ModuleList([])\n",
    "        self.z_heads = torch.nn.ModuleList([])\n",
    "        self.mu_heads = torch.nn.ModuleList([])\n",
    "        \n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_bus_modules_1\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_bus_modules_2\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_gen_modules_1\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_gen_modules_2\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *l_modules\n",
    "        ))\n",
    "        self.z_heads.append(torch.nn.Sequential(\n",
    "            *z_modules\n",
    "        ))\n",
    "        self.mu_heads.append(torch.nn.Sequential(\n",
    "            *mu_modules\n",
    "        ))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Shared_head = self.net(x)\n",
    "        output = []\n",
    "        for head in self.x_l_heads:\n",
    "            output.append(head(Shared_head))\n",
    "        for head in self.z_heads:\n",
    "            output.append(head(torch.cat((output[0], output[1], output[2], output[3]), dim=1)))\n",
    "        for head in self.mu_heads:\n",
    "            output.append(head(output[5]))\n",
    "        return output\n",
    "\n",
    "def create_model_MTL_solo(bus_size, gen_size, z_mu_size, shouldPrint, batch_size=32):\n",
    "    print(\"Creating MTL model for solo use...\")\n",
    "    model = TestMultiTaskNetSolo(bus_size, gen_size, z_mu_size).to(get_device())\n",
    "    if (shouldPrint):\n",
    "        summary(model, (batch_size,bus_size*2))\n",
    "    return model\n",
    "\n",
    "def train_model_MTL_solo(model, train_dataloader, validate_dataloader):\n",
    "    # model = create_model(bus_size, False)\n",
    "    n_epochs = 10\n",
    "    device = get_device()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    loss_per_epoch = []\n",
    "    val_loss_per_epoch = []\n",
    "    acc_per_epoch = []\n",
    "    val_acc_per_epoch = []\n",
    "\n",
    "    lossStruct = torch.nn.L1Loss()\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"MTL Solo | Starting epoch: {epoch+1}\")\n",
    "        model.train(True)\n",
    "        total_training_loss = 0\n",
    "        total_training_acc = 0\n",
    "        best_mse = 0\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            inputs = data[\"x_val\"].to(device=device)\n",
    "            Va_label = data[\"Va\"].to(device=device)\n",
    "            Vm_label = data[\"Vm\"].to(device=device)\n",
    "            Pg_label = data[\"Pg\"].to(device=device)\n",
    "            Qg_label = data[\"Qg\"].to(device=device)\n",
    "            L_label = data[\"Lambda\"].to(device=device)\n",
    "            Z_label = data[\"Z\"].to(device=device)\n",
    "            Mu_label = data[\"Mu\"].to(device=device)\n",
    "            optimizer.zero_grad()\n",
    "            [Va_output, Vm_output, Pg_output, Qg_output, L_output, Z_output, Mu_output] = model(inputs)\n",
    "            epoch_loss = []\n",
    "            x = torch.cat((Va_output, Vm_output, Pg_output, Qg_output, L_output, Z_output, Mu_output), 1)\n",
    "            y = torch.cat((Va_label, Vm_label, Pg_label, Qg_label, L_label, Z_label, Mu_label), 1)\n",
    "            # loss = lossStruct.forward(x, y)\n",
    "            loss = lossStruct(x, y) / 32\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_training_loss += loss\n",
    "            train_acc = torch.sum(x == y)\n",
    "        #     mse = ((torch.pow((x - y), 2)).sum()) / 32\n",
    "        # if best_mse == 0 or best_mse < mse:\n",
    "        #     best_mse = mse\n",
    "        print(\"Training Loss after epoch: \" + str(total_training_loss.item()))\n",
    "        loss_per_epoch.append(total_training_loss.item())\n",
    "        # print(\"Training Acc after epoch: \" + str(best_mse))\n",
    "        # acc_per_epoch.append(best_mse)\n",
    "\n",
    "        total_validation_loss = 0\n",
    "        best_mse = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(validate_dataloader):\n",
    "                inputs = data[\"x_val\"].to(device=device)\n",
    "                Va_label = data[\"Va\"].to(device=device)\n",
    "                Vm_label = data[\"Vm\"].to(device=device)\n",
    "                Pg_label = data[\"Pg\"].to(device=device)\n",
    "                Qg_label = data[\"Qg\"].to(device=device)\n",
    "                L_label = data[\"Lambda\"].to(device=device)\n",
    "                Z_label = data[\"Z\"].to(device=device)\n",
    "                Mu_label = data[\"Mu\"].to(device=device)\n",
    "                [Va_output, Vm_output, Pg_output, Qg_output, L_output, Z_output, Mu_output] = model(inputs)\n",
    "                x = torch.cat((Va_output, Vm_output, Pg_output, Qg_output, L_output, Z_output, Mu_output), 1)\n",
    "                y = torch.cat((Va_label, Vm_label, Pg_label, Qg_label, L_label, Z_label, Mu_label), 1)\n",
    "                loss = lossStruct(x, y) / 32\n",
    "                total_validation_loss += loss\n",
    "            #     mse = ((torch.pow((x - y), 2)).sum()) / 32\n",
    "            # if best_mse == 0 or best_mse < mse:\n",
    "            #     best_mse = mse\n",
    "            print(\"Validation Loss after epoch: \" + str(total_validation_loss.item()))\n",
    "            val_loss_per_epoch.append(total_validation_loss.item())\n",
    "            # print(\"Validation Acc after epoch: \" + str(best_mse))\n",
    "            # val_acc_per_epoch.append(best_mse)\n",
    "                \n",
    "    print(\"Done Training, Loss: \")\n",
    "    print(total_training_loss)\n",
    "    return (model, loss_per_epoch, val_loss_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d496b-6c0e-49dd-9386-c3e2a07492cd",
   "metadata": {},
   "source": [
    "## Neuromancer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "7c7f37d7-bacb-49da-a16a-c243e498bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMultiTaskNetNeuromancer(torch.nn.Module):\n",
    "    def __init__(self, bus_size, gen_size, z_mu_size):\n",
    "        super(TestMultiTaskNetNeuromancer, self).__init__()\n",
    "        # Sahred layers (Input: 28, Output: 58)\n",
    "\n",
    "        layer_1_sizes, layer_x_sizes_1, layer_x_sizes_2, layer_l_sizes, layer_z_sizes, layer_mu_sizes = generate_layers(bus_size, gen_size, z_mu_size)\n",
    "\n",
    "        # print(layer_1_sizes)\n",
    "        # print(layer_x_sizes_1)\n",
    "        # print(layer_x_sizes_2)\n",
    "        # print(layer_l_sizes)\n",
    "        # print(layer_z_sizes)\n",
    "        # print(layer_mu_sizes)\n",
    "\n",
    "        self.device = get_device()\n",
    "        \n",
    "        shared_modules = []\n",
    "        x_bus_modules_1 = []\n",
    "        x_bus_modules_2 = []\n",
    "        x_gen_modules_1 = []\n",
    "        x_gen_modules_2 = []\n",
    "        l_modules = []\n",
    "        z_modules = []\n",
    "        mu_modules = []\n",
    "\n",
    "        for i in range(1, len(layer_1_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                shared_modules.append(torch.nn.Linear(layer_1_sizes[i-1], layer_1_sizes[i]))\n",
    "                # shared_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                shared_modules.append(torch.nn.Linear(layer_1_sizes[i-1], layer_1_sizes[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_1)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_bus_modules_1.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "                # x_bus_modules_1.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_bus_modules_1.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_1)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_bus_modules_2.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "                # x_bus_modules_2.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_bus_modules_2.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_2)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_gen_modules_1.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "                # x_gen_modules_1.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_gen_modules_1.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_2)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_gen_modules_2.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "                # x_gen_modules_2.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_gen_modules_2.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "\n",
    "        for i in range(1, len(layer_l_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                l_modules.append(torch.nn.Linear(layer_l_sizes[i-1], layer_l_sizes[i]))\n",
    "                # l_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                l_modules.append(torch.nn.Linear(layer_l_sizes[i-1], layer_l_sizes[i]))\n",
    "\n",
    "        for i in range(1, len(layer_z_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                z_modules.append(torch.nn.Linear(layer_z_sizes[i-1], layer_z_sizes[i]))\n",
    "                # z_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                z_modules.append(torch.nn.Linear(layer_z_sizes[i-1], layer_z_sizes[i]))\n",
    "\n",
    "        for i in range(1, len(layer_mu_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                mu_modules.append(torch.nn.Linear(layer_mu_sizes[i-1], layer_mu_sizes[i]))\n",
    "                # mu_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                mu_modules.append(torch.nn.Linear(layer_mu_sizes[i-1], layer_mu_sizes[i]))\n",
    "        \n",
    "        self.net = torch.nn.Sequential(*shared_modules)\n",
    "        \n",
    "        self.n_features = bus_size*2\n",
    "        self.net.fc = torch.nn.Identity()\n",
    "\n",
    "        self.x_l_heads = torch.nn.ModuleList([])\n",
    "        self.z_heads = torch.nn.ModuleList([])\n",
    "        self.mu_heads = torch.nn.ModuleList([])\n",
    "        \n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_bus_modules_1\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_bus_modules_2\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_gen_modules_1\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_gen_modules_2\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *l_modules\n",
    "        ))\n",
    "        self.z_heads.append(torch.nn.Sequential(\n",
    "            *z_modules\n",
    "        ))\n",
    "        self.mu_heads.append(torch.nn.Sequential(\n",
    "            *mu_modules\n",
    "        ))\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=1).to(self.device)\n",
    "        Shared_head = self.net(x)\n",
    "        output = []\n",
    "        for head in self.x_l_heads:\n",
    "            output.append(head(Shared_head))\n",
    "        for head in self.z_heads:\n",
    "            output.append(head(torch.cat((output[0], output[1], output[2], output[3]), dim=1)))\n",
    "        for head in self.mu_heads:\n",
    "            output.append(head(output[5]))\n",
    "        # return output\n",
    "        return [x.to(\"cpu\") for x in output]\n",
    "\n",
    "def create_model_MTL_neuromancer(bus_size, gen_size, z_mu_size, shouldPrint, batch_size=32):\n",
    "    print(\"Creating MTL model for neuromancer...\")\n",
    "    model = TestMultiTaskNetNeuromancer(bus_size, gen_size, z_mu_size).to(get_device())\n",
    "    if (shouldPrint):\n",
    "        summary(model, (batch_size,bus_size*2))\n",
    "    return model\n",
    "\n",
    "def get_Vm_min():\n",
    "    fileStr = get_case_as_string(\"generated_cases/\"+os.listdir(\"generated_cases\")[0])\n",
    "    pattern = r\"mpc.bus = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    bus_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    Vm_mins = []\n",
    "    for i in range(1, len(bus_data)-1):\n",
    "        current_row = bus_data[i].split(\"\\t\")\n",
    "        Vm_mins.append(float(current_row[-1].replace(\";\", \"\")))\n",
    "    return Vm_mins\n",
    "\n",
    "def get_Vm_max():\n",
    "    fileStr = get_case_as_string(\"generated_cases/\"+os.listdir(\"generated_cases\")[0])\n",
    "    pattern = r\"mpc.bus = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    bus_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    Vm_maxs = []\n",
    "    for i in range(1, len(bus_data)-1):\n",
    "        current_row = bus_data[i].split(\"\\t\")\n",
    "        Vm_maxs.append(float(current_row[-2]))\n",
    "    return Vm_maxs\n",
    "    \n",
    "def get_Qg_min():\n",
    "    fileStr = get_case_as_string(\"generated_cases/\"+os.listdir(\"generated_cases\")[0])\n",
    "    pattern = r\"mpc.gen = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    gen_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    Qg_mins = []\n",
    "    for i in range(1, len(gen_data)-1):\n",
    "        current_row = gen_data[i].split(\"\\t\")\n",
    "        Qg_mins.append(float(current_row[4]))\n",
    "    return Qg_mins\n",
    "    \n",
    "def get_Qg_max():\n",
    "    fileStr = get_case_as_string(\"generated_cases/\"+os.listdir(\"generated_cases\")[0])\n",
    "    pattern = r\"mpc.gen = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    gen_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    Qg_maxs = []\n",
    "    for i in range(1, len(gen_data)-1):\n",
    "        current_row = gen_data[i].split(\"\\t\")\n",
    "        Qg_maxs.append(float(current_row[3]))\n",
    "    return Qg_maxs\n",
    "\n",
    "def get_Pg_min():\n",
    "    fileStr = get_case_as_string(\"generated_cases/\"+os.listdir(\"generated_cases\")[0])\n",
    "    pattern = r\"mpc.gen = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    gen_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    Pg_mins = []\n",
    "    for i in range(1, len(gen_data)-1):\n",
    "        current_row = gen_data[i].split(\"\\t\")\n",
    "        Pg_mins.append(float(current_row[9]))\n",
    "    return Pg_mins\n",
    "\n",
    "def get_Pg_max():\n",
    "    fileStr = get_case_as_string(\"generated_cases/\"+os.listdir(\"generated_cases\")[0])\n",
    "    pattern = r\"mpc.gen = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    gen_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    Pg_maxs = []\n",
    "    for i in range(1, len(gen_data)-1):\n",
    "        current_row = gen_data[i].split(\"\\t\")\n",
    "        Pg_maxs.append(float(current_row[8]))\n",
    "    return Pg_maxs\n",
    "\n",
    "def create_neuromancer_problem(func):\n",
    "    sol_map = Node(func, [\"Pd\", \"Qd\"], [\"x\"], name='map')\n",
    "\n",
    "    Pd = variable(\"Pd\")\n",
    "    Qd = variable(\"Qd\")\n",
    "    Va = variable(\"Va\")\n",
    "    Vm = variable(\"Vm\")\n",
    "    Pg = variable(\"Pg\")\n",
    "    Qg = variable(\"Qg\")\n",
    "    Lambda = variable(\"Lambda\")\n",
    "    Z = variable(\"Z\")\n",
    "    Mu = variable(\"Mu\")\n",
    "    Va_i = variable(\"x\")[0]\n",
    "    Vm_i = variable(\"x\")[1]\n",
    "    Pg_i = variable(\"x\")[2]\n",
    "    Qg_i = variable(\"x\")[3]\n",
    "    Lambda_i = variable(\"x\")[4]\n",
    "    Z_i = variable(\"x\")[5]\n",
    "    Mu_i = variable(\"x\")[6]\n",
    "    \n",
    "    # Objective function\n",
    "    f1 = torch.abs(Vm-Vm_i) + torch.abs(Va-Va_i)\n",
    "    f2 = torch.abs(Pg-Pg_i) + torch.abs(Qg-Qg_i)\n",
    "    f3 = torch.abs(Lambda - Lambda_i)\n",
    "    f4 = torch.abs(Z - Z_i)\n",
    "    f5 = torch.abs(Mu - Mu_i)\n",
    "    obj1 = f1.minimize(weight=0.35, name='obj1')\n",
    "    obj2 = f2.minimize(weight=0.35, name='obj2')\n",
    "    obj3 = f3.minimize(weight=0.10, name='obj3')\n",
    "    obj4 = f4.minimize(weight=0.10, name='obj4')\n",
    "    obj5 = f5.minimize(weight=0.10, name='obj5')\n",
    "    \n",
    "    # Constraints\n",
    "    Q_con = 100.  # constraint penalty weights\n",
    "    \n",
    "    # con_1 = Q_con*(V_min <= Vm_i <= V_max)\n",
    "    # con_1.name = 'c1'\n",
    "    \n",
    "    # constraints = [con_1]\n",
    "    constraints = []\n",
    "\n",
    "    for index, (min, max) in enumerate(zip(get_Vm_min(), get_Vm_max())):\n",
    "        con = Q_con*(min <= Vm_i[:, [index]] <= max)\n",
    "        con.name= 'c1_'+str(index)\n",
    "        constraints.append(con)\n",
    "    \n",
    "    for index, (min, max) in enumerate(zip(get_Qg_min(), get_Qg_max())):\n",
    "        con = Q_con*(min <= Qg_i[:, [index]] <= max)\n",
    "        con.name = 'c2_'+str(index)\n",
    "        constraints.append(con)\n",
    "    \n",
    "    for index, (min, max) in enumerate(zip(get_Pg_min(), get_Pg_max())):\n",
    "        con = Q_con*(min <= Pg_i[:, [index]] <= max)\n",
    "        con.name = 'c3_'+str(index)\n",
    "        constraints.append(con)\n",
    "    \n",
    "    \n",
    "    # constrained optimization problem construction\n",
    "    objectives = [obj1, obj2, obj3, obj4, obj5]\n",
    "    components = [sol_map]\n",
    "    \n",
    "    # create penalty method loss function\n",
    "    loss = PenaltyLoss(objectives, constraints)\n",
    "    # construct constrained optimization problem\n",
    "    return Problem(components, loss)\n",
    "\n",
    "def prepare_trainer(problem, train_data, dev_data, test_data):\n",
    "    lr = 0.0001      # step size for gradient descent\n",
    "    epochs = 10    # number of training epochs\n",
    "    warmup = 5    # number of epochs to wait before enacting early stopping policy\n",
    "    patience = 5  # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "\n",
    "    optimizer = torch.optim.AdamW(problem.parameters(), lr=lr)\n",
    "    \n",
    "    # define trainer\n",
    "    return Trainer(\n",
    "        problem,\n",
    "        train_data,\n",
    "        dev_data,\n",
    "        test_data,\n",
    "        optimizer,\n",
    "        epochs=epochs,\n",
    "        patience=patience,\n",
    "        warmup=warmup)\n",
    "\n",
    "def train_neuromancer_model(trainer):\n",
    "    best_model = trainer.train()\n",
    "    best_outputs = trainer.test(best_model)\n",
    "    return best_model, best_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d179d2c-124e-4af3-b27e-c15ed6351e42",
   "metadata": {},
   "source": [
    "## Main Pipeline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "e41e638f-a6fa-47cb-8f15-a4e440088ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline(case_file, \n",
    "                  total_number_of_cases=10000, \n",
    "                  delta=0.1, \n",
    "                  test_percent=0.2, \n",
    "                  activations=[None, None, None, None, None],\n",
    "                  dropouts=[(False, 0),(False, 0),(False, 0),(False, 0),(False, 0)],\n",
    "                  engine=\"matlab\"):\n",
    "    fileStr = get_case_as_string(case_file)\n",
    "    bus = (get_bus_count(fileStr), get_gen_count(fileStr))\n",
    "    create_test_cases(fileStr, total_number_of_cases, delta)\n",
    "    # # label_cases(engine)\n",
    "    if not os.path.exists(\"mips_results\"):\n",
    "        print(\"Label cases to continue\")\n",
    "        return\n",
    "    bus_dir = f\"generated_cases/\"\n",
    "    MTL_solo_train = None\n",
    "    MTL_solo_val = None\n",
    "    neuromancer_train_loader = None\n",
    "    neuromancer_dev_loader = None\n",
    "    neuromancer_test_loader = None\n",
    "    output_size = None\n",
    "    MTL_solo_train, MTL_solo_val = get_MTL_solo_data(bus[0], bus[1])\n",
    "    neuromancer_train_loader, neuromancer_dev_loader, neuromancer_test_loader, output_size = get_neuromancer_data(bus[0], bus[1])\n",
    "\n",
    "    MTL_solo_model = create_model_MTL_solo(bus[0], bus[1], get_z_mu_size(), False, 32)\n",
    "    trained_MTL_solo_model, MTL_solo_loss_per_epoch, MTL_solo_val_loss_per_epoch = train_model_MTL_solo(MTL_solo_model, MTL_solo_train, MTL_solo_val)\n",
    "    \n",
    "    MTL_in_neuromancer_model = create_model_MTL_neuromancer(bus[0], bus[1], get_z_mu_size(), False, 32)\n",
    "    neuromancer_problem = create_neuromancer_problem(MTL_in_neuromancer_model)\n",
    "    neuromancer_trainer = prepare_trainer(neuromancer_problem, neuromancer_train_loader, neuromancer_dev_loader, neuromancer_test_loader)\n",
    "    neuromancer_best_model, neuromancer_best_outputs = train_neuromancer_model(neuromancer_trainer)\n",
    "    neuromancer_problem.load_state_dict(neuromancer_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "7aa08ee3-2130-463f-bcd2-2d2e3e4f01a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases already generated, skipping...\n",
      "Data Loaders for MTL solo found, loading from .pth files\n",
      "\n",
      "Data Loaders for MTL in Neuromancer found, loading from .pth files\n",
      "\n",
      "Creating MTL model for solo use...\n",
      "MTL Solo | Starting epoch: 1\n",
      "Training Loss after epoch: 2725.273681640625\n",
      "Validation Loss after epoch: 686.6591186523438\n",
      "MTL Solo | Starting epoch: 2\n",
      "Training Loss after epoch: 2724.23828125\n",
      "Validation Loss after epoch: 686.18896484375\n",
      "MTL Solo | Starting epoch: 3\n",
      "Training Loss after epoch: 2720.246826171875\n",
      "Validation Loss after epoch: 684.2876586914062\n",
      "MTL Solo | Starting epoch: 4\n",
      "Training Loss after epoch: 2699.376953125\n",
      "Validation Loss after epoch: 676.1289672851562\n",
      "MTL Solo | Starting epoch: 5\n",
      "Training Loss after epoch: 2658.9609375\n",
      "Validation Loss after epoch: 660.732421875\n",
      "MTL Solo | Starting epoch: 6\n",
      "Training Loss after epoch: 2554.10986328125\n",
      "Validation Loss after epoch: 620.3870239257812\n",
      "MTL Solo | Starting epoch: 7\n",
      "Training Loss after epoch: 2281.24853515625\n",
      "Validation Loss after epoch: 512.022216796875\n",
      "MTL Solo | Starting epoch: 8\n",
      "Training Loss after epoch: 1593.8594970703125\n",
      "Validation Loss after epoch: 257.21173095703125\n",
      "MTL Solo | Starting epoch: 9\n",
      "Training Loss after epoch: 620.0022583007812\n",
      "Validation Loss after epoch: 113.22044372558594\n",
      "MTL Solo | Starting epoch: 10\n",
      "Training Loss after epoch: 375.1548156738281\n",
      "Validation Loss after epoch: 78.46932983398438\n",
      "Done Training, Loss: \n",
      "tensor(375.1548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Creating MTL model for neuromancer...\n",
      "epoch: 0  train_loss: 70776.0625\n",
      "epoch: 1  train_loss: 347.262451171875\n",
      "epoch: 2  train_loss: 142.063232421875\n",
      "epoch: 3  train_loss: 87.63787078857422\n",
      "epoch: 4  train_loss: 97.80463409423828\n",
      "epoch: 5  train_loss: 87.0591812133789\n",
      "epoch: 6  train_loss: 86.21329498291016\n",
      "epoch: 7  train_loss: 85.41777801513672\n",
      "epoch: 8  train_loss: 82.97209167480469\n",
      "epoch: 9  train_loss: 83.07129669189453\n"
     ]
    }
   ],
   "source": [
    "full_pipeline(\"case14.m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b65e7f-867a-4337-9e1d-371bc36c241a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9624d34-ee28-4b2d-b149-8a15028cbb49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
