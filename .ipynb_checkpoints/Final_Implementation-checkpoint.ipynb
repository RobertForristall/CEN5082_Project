{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceaf3a41-33bf-4d93-84fc-4db9dfd846cc",
   "metadata": {},
   "source": [
    "# Final Pipeline\n",
    "## Author: Robert Forristall\n",
    "\n",
    "This document implements the final complete implementation of my work for FIU Spring 2024 CEN 5082 Project: Using Neuromancer tool to integrate physical constraints for an HPC Application (Power Grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "59ec5212-8bdf-4e19-bb0e-a9d45b6185d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Generation\n",
    "import os\n",
    "import re\n",
    "import random as rand\n",
    "import math\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset Preparation\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import string\n",
    "import numpy as np\n",
    "from neuromancer.dataset import DictDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd81fae-3e3c-4eb7-8b2b-a4beb5fed56f",
   "metadata": {},
   "source": [
    "## Case Generation\n",
    "\n",
    "This section of code declares all of the functions that are used for the generation of new cases that can be used to train a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "54319458-6491-4ba7-9a40-1bc5733fb94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleLowBound(pd, t):\n",
    "    return (1-t)*pd;\n",
    "\n",
    "def sampleHighBound(pd, t):\n",
    "    return (1+t)*pd;\n",
    "\n",
    "def sampleLoadValue(pd, t):\n",
    "    if (pd != 0):\n",
    "        low = sampleLowBound(pd, t)*100\n",
    "        high = sampleHighBound(pd, t)*100\n",
    "        if (low < high):\n",
    "            return rand.randrange(math.floor(low), math.ceil(high))/100\n",
    "        else:\n",
    "            return rand.randrange(math.floor(high), math.ceil(low))/100\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_function_name(fileStr, index):\n",
    "    return re.findall(\"function .* = .*\", fileStr)[0] + f\"_{index}\"\n",
    "\n",
    "def get_mpc_version(fileStr):\n",
    "    return re.findall(\"mpc.version = '\\d';\", fileStr)[0]\n",
    "\n",
    "def get_mpc_base(fileStr):\n",
    "    return re.findall(\"mpc.baseMVA = \\d*;\", fileStr)[0]\n",
    "\n",
    "def get_bus_data(fileStr, delta):\n",
    "    pattern = r\"mpc.bus = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    bus_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    return_str = \"\"\n",
    "    return_str += bus_data[0] + \"\\n\"\n",
    "    for i in range(1, len(bus_data)-1):\n",
    "        current_row = bus_data[i].split(\"\\t\")\n",
    "        current_row[3] = str(sampleLoadValue(float(current_row[3]), delta))\n",
    "        current_row[4] = str(sampleLoadValue(float(current_row[4]), delta))\n",
    "        return_str += \"\\t\".join(current_row) + \"\\n\"\n",
    "    return_str += bus_data[-1] + \"\\n\"\n",
    "    return return_str\n",
    "\n",
    "def get_gen_data(fileStr):\n",
    "    pattern = r\"mpc.gen = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    gen_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    return_str = \"\"\n",
    "    return_str += gen_data[0] + \"\\n\"\n",
    "    for i in range(1, len(gen_data)-1):\n",
    "        return_str += gen_data[i] + \"\\n\"\n",
    "    return_str += gen_data[-1] + \"\\n\"\n",
    "    return return_str\n",
    "\n",
    "def get_branch_data(fileStr):\n",
    "    pattern = r\"mpc.branch = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    branch_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    return_str = \"\"\n",
    "    return_str += branch_data[0] + \"\\n\"\n",
    "    for i in range(1, len(branch_data)-1):\n",
    "        return_str += branch_data[i] + \"\\n\"\n",
    "    return_str += branch_data[-1] + \"\\n\"\n",
    "    return return_str\n",
    "\n",
    "def get_gencost_data(fileStr):\n",
    "    pattern = r\"mpc.gencost = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    gencost_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    return_str = \"\"\n",
    "    return_str += gencost_data[0] + \"\\n\"\n",
    "    for i in range(1, len(gencost_data)-1):\n",
    "        return_str += gencost_data[i] + \"\\n\"\n",
    "    return_str += gencost_data[-1] + \"\\n\"\n",
    "    return return_str\n",
    "\n",
    "def get_full_case(fileStr, index, delta):\n",
    "    case_str = \"\\n\\n\".join([\n",
    "        get_function_name(fileStr, index),\n",
    "        get_mpc_version(fileStr),\n",
    "        get_mpc_base(fileStr),\n",
    "        get_bus_data(fileStr, delta),\n",
    "        get_gen_data(fileStr),\n",
    "        get_branch_data(fileStr),\n",
    "        get_gencost_data(fileStr)\n",
    "    ])\n",
    "    return case_str\n",
    "\n",
    "def get_case_as_string(case_file):\n",
    "    with open(case_file, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "def create_test_cases(fileStr, total_number_of_cases, delta):\n",
    "    if not os.path.exists(\"generated_cases\"):\n",
    "        print(\"Generating New Cases...\")\n",
    "        os.mkdir(\"generated_cases\")\n",
    "        for i in tqdm(range(total_number_of_cases)):\n",
    "            with open(f\"generated_cases/GeneratedCase{i}.m\", \"w\") as file:\n",
    "                file.write(get_full_case(fileStr, i, delta))\n",
    "    else:\n",
    "        print(\"Cases already generated, skipping...\")\n",
    "        # shutil.rmtree(\"generated_cases\")\n",
    "        # os.mkdir(\"generated_cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b027667-c91e-49cf-a541-e7cb104856c8",
   "metadata": {},
   "source": [
    "## Run generated cases through MIPS\n",
    "\n",
    "### In future iterations this stage will be automated by implementing MIPS in python\n",
    "\n",
    "Run the generated files through MIPS and save the following for each case in its own excel within its own directory inside 'mips_results/' + the variable name lowercase:\n",
    "- Optimization vector X (Va, Vm, Pg, Qg), saved in 'mips_results/x'\n",
    "- Langrangian equality metric Lambda, saved in 'mips_results/lambda'\n",
    "- Slack variable vector Z, saved in 'mips_results/z'\n",
    "- Langrangian inequality metric Mu, saved in 'mips_results/mu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de63ab71-de70-495e-b499-ce629f8671c6",
   "metadata": {},
   "source": [
    "## Prepare Train/Test Data (MTL Solo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0e0c1b48-ddcc-4bfa-aa90-d0cce5d86902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bus_count(fileStr):\n",
    "    pattern = r\"mpc.bus = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    bus_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    return len(bus_data)-2\n",
    "\n",
    "def get_gen_count(fileStr):\n",
    "    pattern = r\"mpc.gen = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    gen_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    return len(gen_data)-2\n",
    "\n",
    "def get_pds_from_case(file):\n",
    "    fileStr = get_case_as_string(file)\n",
    "    pattern = r\"mpc.bus = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    bus_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    pds = []\n",
    "    for i in range(1, len(bus_data)-1):\n",
    "        current_row = bus_data[i].split(\"\\t\")\n",
    "        pds.append(current_row[3])\n",
    "    return pds\n",
    "\n",
    "def get_qds_from_case(file):\n",
    "    fileStr = get_case_as_string(file)\n",
    "    pattern = r\"mpc.bus = \\[[\\n|\\d|\\t|;|.|-]*\\];\"\n",
    "    bus_data = re.findall(pattern, fileStr)[0].split(\"\\n\")\n",
    "    qds = []\n",
    "    for i in range(1, len(bus_data)-1):\n",
    "        current_row = bus_data[i].split(\"\\t\")\n",
    "        qds.append(current_row[4])\n",
    "    return qds\n",
    "\n",
    "def build_pds_csv(bus_size):\n",
    "    print(\"Building pds...\")\n",
    "    pd_data = pd.DataFrame(columns = [f\"bus_{x+1}\" for x in range(bus_size)]);\n",
    "    for file in tqdm(os.listdir(\"generated_cases\")):\n",
    "        pd_data.loc[len(pd_data.index)] = get_pds_from_case(\"generated_cases/\"+file)\n",
    "    pd_data.to_csv(\"bus_Pds.csv\", index=False)\n",
    "\n",
    "def build_qds_csv(bus_size):\n",
    "    print(\"Building qds...\")\n",
    "    qd_data = pd.DataFrame(columns = [f\"bus_{x+1}\" for x in range(bus_size)]);\n",
    "    for file in tqdm(os.listdir(\"generated_cases\")):\n",
    "        qd_data.loc[len(qd_data.index)] = get_qds_from_case(\"generated_cases/\"+file)\n",
    "    qd_data.to_csv(\"bus_Qds.csv\", index=False)\n",
    "\n",
    "def load_data_MTL(bus_dir, bus_size, gen_size):\n",
    "    # Load Bus Input Data\n",
    "    if not os.path.exists(\"bus_Pds.csv\"):\n",
    "        build_pds_csv(bus_size)\n",
    "    if not os.path.exists(\"bus_Qds.csv\"):\n",
    "        build_qds_csv(bus_size)\n",
    "    Pds = pd.read_csv(bus_dir + 'bus_Pds.csv').to_numpy()\n",
    "    Qds = pd.read_csv(bus_dir + 'bus_Qds.csv').to_numpy()\n",
    "    input_data = []\n",
    "    \n",
    "    # Load Bus Label Data\n",
    "    Va = []\n",
    "    Vm = []\n",
    "    Pg = []\n",
    "    Qg = []\n",
    "    Lambda = []\n",
    "    Z = []\n",
    "    Mu = []\n",
    "    for file in tqdm(os.listdir(bus_dir + 'mips_results/x')):\n",
    "        case_id = int(file.strip(string.ascii_letters + \".\"))\n",
    "        input_data.append(np.concatenate((Pds[case_id], Qds[case_id]), axis=0))\n",
    "        x_data = pd.read_csv(bus_dir + 'mips_results/x/' + file, header=None).to_numpy().flatten()\n",
    "        Va.append(x_data[0:bus_size])\n",
    "        Vm.append(x_data[bus_size:bus_size*2])\n",
    "        Pg.append(x_data[bus_size*2:(bus_size*2) + gen_size])\n",
    "        Qg.append(x_data[(bus_size*2) + gen_size:(bus_size*2) + (gen_size*2)])\n",
    "        # Lambda.append(json.load(open('mips_results/lambda/myCase' + str(case_id) + '.json'))[\"eqnonlin\"])\n",
    "        Lambda.append(pd.read_csv(bus_dir + 'mips_results/lambda/' + file, header=None).to_numpy().flatten())\n",
    "        Z.append(pd.read_csv(bus_dir + 'mips_results/z/' + file, header=None).to_numpy().flatten())\n",
    "        Mu.append(pd.read_csv(bus_dir + 'mips_results/mu/' + file, header=None).to_numpy().flatten())\n",
    "    return (input_data, Va, Vm, Pg, Qg, Lambda, Z, Mu)\n",
    "\n",
    "# Craate Dataset\n",
    "class CustomOpfMultiTaskDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_data, Va, Vm, Pg, Qg, Lambda, Z, Mu):\n",
    "        self.x_data = np.array(x_data).astype(np.float32)\n",
    "        self.Va = np.array(Va).astype(np.float32)\n",
    "        self.Vm = np.array(Vm).astype(np.float32)\n",
    "        self.Pg = np.array(Pg).astype(np.float32)\n",
    "        self.Qg = np.array(Qg).astype(np.float32)\n",
    "        self.Lambda = np.array(Lambda).astype(np.float32)\n",
    "        self.Z = np.array(Z).astype(np.float32)\n",
    "        self.Mu = np.array(Mu).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'x_val': self.x_data[index],\n",
    "            'Va': self.Va[index],\n",
    "            'Vm': self.Vm[index],\n",
    "            'Pg': self.Pg[index],\n",
    "            'Qg': self.Qg[index],\n",
    "            'Lambda': self.Lambda[index],\n",
    "            'Z': self.Z[index],\n",
    "            'Mu': self.Mu[index]\n",
    "        }\n",
    "\n",
    "def create_data_loaders(split_index, batch_size, data_tuple, bus_size):\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        CustomOpfMultiTaskDataset(\n",
    "            data_tuple[0][0:split_index],\n",
    "            data_tuple[1][0:split_index],\n",
    "            data_tuple[2][0:split_index],\n",
    "            data_tuple[3][0:split_index],\n",
    "            data_tuple[4][0:split_index],\n",
    "            data_tuple[5][0:split_index],\n",
    "            data_tuple[6][0:split_index],\n",
    "            data_tuple[7][0:split_index],\n",
    "        ), shuffle=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    validate_dataloader = DataLoader(\n",
    "        CustomOpfMultiTaskDataset(\n",
    "            data_tuple[0][split_index:],\n",
    "            data_tuple[1][split_index:],\n",
    "            data_tuple[2][split_index:],\n",
    "            data_tuple[3][split_index:],\n",
    "            data_tuple[4][split_index:],\n",
    "            data_tuple[5][split_index:],\n",
    "            data_tuple[6][split_index:],\n",
    "            data_tuple[7][split_index:],\n",
    "        ), shuffle=False, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return (train_dataloader, validate_dataloader)\n",
    "\n",
    "def get_MTL_solo_data(bus_size, gen_size):\n",
    "    if (not os.path.exists(f\"trainData.pth\") or not os.path.exists(f\"validData.pth\")):\n",
    "        print(\"Data Loaders not found, loading from raw data\\n\")\n",
    "        (train_dataloader, validate_dataloader) = create_data_loaders(8000, 32, load_data_MTL(\"\", bus_size, gen_size), bus_size)\n",
    "        torch.save(train_dataloader, \"trainData.pth\")\n",
    "        torch.save(validate_dataloader, \"validData.pth\")\n",
    "    else:\n",
    "        print(\"Data Loaders found, loading from .pth files\\n\")\n",
    "        train_dataloader = torch.load(\"trainData.pth\")\n",
    "        validate_dataloader = torch.load(\"validData.pth\")\n",
    "\n",
    "    return train_dataloader, validate_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f279fa-c209-4ad1-b4de-a2d20c32d7bc",
   "metadata": {},
   "source": [
    "## Prepare Train/Test Data (MTL With Neuromancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ce185702-6dc3-4f1c-8fab-aabb658b69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_dict(Pds, Qds, Va, Vm, Pg, Qg, Lambda, Z, Mu, start_index, end_index):\n",
    "        return {\n",
    "        \"Pd\": torch.tensor(Pds[start_index:end_index], dtype=torch.float32),\n",
    "        \"Qd\": torch.tensor(Qds[start_index:end_index], dtype=torch.float32),\n",
    "        \"Va\": torch.tensor(Va[start_index:end_index], dtype=torch.float32),\n",
    "        \"Vm\": torch.tensor(Vm[start_index:end_index], dtype=torch.float32),\n",
    "        \"Pg\": torch.tensor(Pg[start_index:end_index], dtype=torch.float32),\n",
    "        \"Qg\": torch.tensor(Qg[start_index:end_index], dtype=torch.float32),\n",
    "        \"Lambda\": torch.tensor(Lambda[start_index:end_index], dtype=torch.float32),\n",
    "        \"Z\": torch.tensor(Z[start_index:end_index], dtype=torch.float32),\n",
    "        \"Mu\": torch.tensor(Mu[start_index:end_index], dtype=torch.float32)\n",
    "    }\n",
    "\n",
    "def load_data(bus_dir):\n",
    "    train_data = torch.load(bus_dir + \"neuromancerTrainData.pth\")\n",
    "    dev_data = torch.load(bus_dir + \"neuromancerDevData.pth\")\n",
    "    test_data = torch.load(bus_dir + \"neuromancerTestData.pth\")\n",
    "    out_size = pd.read_csv(bus_dir + \"output_size.csv\", header=None).to_numpy()[0][0]\n",
    "\n",
    "    return train_data, dev_data, test_data, out_size\n",
    "    \n",
    "\n",
    "def create_neuromancer_data(bus_dir, bus_size, gen_size, train_percent, dev_percent, test_percent):\n",
    "    # Load Bus Input Data\n",
    "    if not os.path.exists(\"bus_Pds.csv\"):\n",
    "        build_pds_csv(bus_size)\n",
    "    if not os.path.exists(\"bus_Qds.csv\"):\n",
    "        build_qds_csv(bus_size)\n",
    "    Pds = pd.read_csv(bus_dir + 'bus_Pds.csv').to_numpy()\n",
    "    Qds = pd.read_csv(bus_dir + 'bus_Qds.csv').to_numpy()\n",
    "    input_data = []\n",
    "\n",
    "    num_of_cases = Pds.shape[0]\n",
    "    train_end_index = int(num_of_cases*train_percent)\n",
    "    dev_end_index = int(train_end_index + (num_of_cases*dev_percent))\n",
    "    test_end_index = num_of_cases\n",
    "    print(train_end_index)\n",
    "    print(dev_end_index)\n",
    "\n",
    "    out_size = 0\n",
    "    \n",
    "    #Load Bus Label Data\n",
    "    Va = []\n",
    "    Vm = []\n",
    "    Pg = []\n",
    "    Qg = []\n",
    "    Lambda = []\n",
    "    Z = []\n",
    "    Mu = []\n",
    "    for file in tqdm(os.listdir(bus_dir + 'mips_results/x')):\n",
    "        case_id = int(file.strip(string.ascii_letters + \".\"))\n",
    "        # input_data.append(np.concatenate((Pds[case_id], Qds[case_id]), axis=0))\n",
    "        x_data = pd.read_csv(bus_dir + 'mips_results/x/' + file, header=None).to_numpy().flatten()\n",
    "        Va.append(x_data[0:bus_size])\n",
    "        Vm.append(x_data[bus_size:bus_size*2])\n",
    "        Pg.append(x_data[bus_size*2:(bus_size*2) + gen_size])\n",
    "        Qg.append(x_data[(bus_size*2) + gen_size:(bus_size*2) + (gen_size*2)])\n",
    "        # Lambda.append(json.load(open('mips_results/lambda/myCase' + str(case_id) + '.json'))[\"eqnonlin\"])\n",
    "        lambda_data = pd.read_csv(bus_dir + 'mips_results/lambda/' + file, header=None).to_numpy().flatten()\n",
    "        Lambda.append(lambda_data)\n",
    "        z_data = pd.read_csv(bus_dir + 'mips_results/z/' + file, header=None).to_numpy().flatten()\n",
    "        Z.append(z_data)\n",
    "        mu_data = pd.read_csv(bus_dir + 'mips_results/mu/' + file, header=None).to_numpy().flatten()\n",
    "        Mu.append(mu_data)\n",
    "        if out_size == 0:\n",
    "            out_size = x_data.shape[0] + lambda_data.shape[0] + z_data.shape[0] + mu_data.shape[0]\n",
    "    # return (input_data, Va, Vm, Pg, Qg, Lambda, Z, Mu)\n",
    "\n",
    "    train_dict = convert_data_to_dict(Pds, Qds, Va, Vm, Pg, Qg, Lambda, Z, Mu, 0, train_end_index)\n",
    "    dev_dict = convert_data_to_dict(Pds, Qds, Va, Vm, Pg, Qg, Lambda, Z, Mu, train_end_index, dev_end_index)\n",
    "    test_dict = convert_data_to_dict(Pds, Qds, Va, Vm, Pg, Qg, Lambda, Z, Mu, dev_end_index, test_end_index)\n",
    "    \n",
    "    train_dict = DictDataset(train_dict, name='train')\n",
    "    dev_dict = DictDataset(dev_dict, name='dev')\n",
    "    test_dict = DictDataset(test_dict, name='test')\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dict, batch_size=32, num_workers=0,\n",
    "                                               collate_fn=train_dict.collate_fn, shuffle=True)\n",
    "    dev_loader = torch.utils.data.DataLoader(dev_dict, batch_size=32, num_workers=0,\n",
    "                                             collate_fn=dev_dict.collate_fn, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dict, batch_size=32, num_workers=0,\n",
    "                                             collate_fn=test_dict.collate_fn, shuffle=True)\n",
    "    \n",
    "    return train_loader, dev_loader, test_loader, out_size\n",
    "\n",
    "def get_neuromancer_data(bus_size, gen_size):\n",
    "    bus_dir = f\"generated_cases\"\n",
    "    train_loader = None\n",
    "    dev_loader = None\n",
    "    test_loader = None\n",
    "    if (not os.path.exists(\"neuromancerTrainData.pth\")):\n",
    "        train_data, dev_data, test_data, out_size = create_neuromancer_data(f\"\", bus_size, gen_size, 0.7, 0.15, 0.15)\n",
    "        torch.save(train_data, \"neuromancerTrainData.pth\")\n",
    "        torch.save(dev_data, \"neuromancerDevData.pth\")\n",
    "        torch.save(test_data, \"neuromancerTestData.pth\")\n",
    "        df = pd.DataFrame(columns=['output_size'])\n",
    "        df.loc[len(df.index)] = [out_size]\n",
    "        df.to_csv(\"output_size.csv\", header=False, index=False)\n",
    "    else:\n",
    "        train_data, dev_data, test_data, out_size = load_data(bus_dir)\n",
    "\n",
    "    return train_data, dev_data, test_data, out_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2e453-959f-4e63-9a4d-f5ca078fac07",
   "metadata": {},
   "source": [
    "## Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cb6b9f1f-8675-4571-8bd1-edf990281430",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMultiTaskNetSolo(torch.nn.Module):\n",
    "    def __init__(self, bus_size, gen_size, z_mu_size):\n",
    "        super(TestMultiTaskNetV2, self).__init__()\n",
    "        # Sahred layers (Input: 28, Output: 58)\n",
    "\n",
    "        layer_1_sizes, layer_x_sizes_1, layer_x_sizes_2, layer_l_sizes, layer_z_sizes, layer_mu_sizes = generate_layers(bus_size, gen_size, z_mu_size)\n",
    "\n",
    "        print(layer_1_sizes)\n",
    "        print(layer_x_sizes_1)\n",
    "        print(layer_x_sizes_2)\n",
    "        print(layer_l_sizes)\n",
    "        print(layer_z_sizes)\n",
    "        print(layer_mu_sizes)\n",
    "        \n",
    "        shared_modules = []\n",
    "        x_bus_modules_1 = []\n",
    "        x_bus_modules_2 = []\n",
    "        x_gen_modules_1 = []\n",
    "        x_gen_modules_2 = []\n",
    "        l_modules = []\n",
    "        z_modules = []\n",
    "        mu_modules = []\n",
    "\n",
    "        for i in range(1, len(layer_1_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                shared_modules.append(torch.nn.Linear(layer_1_sizes[i-1], layer_1_sizes[i]))\n",
    "                # shared_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                shared_modules.append(torch.nn.Linear(layer_1_sizes[i-1], layer_1_sizes[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_1)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_bus_modules_1.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "                # x_bus_modules_1.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_bus_modules_1.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_1)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_bus_modules_2.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "                # x_bus_modules_2.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_bus_modules_2.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_2)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_gen_modules_1.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "                # x_gen_modules_1.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_gen_modules_1.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_2)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_gen_modules_2.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "                # x_gen_modules_2.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_gen_modules_2.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "\n",
    "        for i in range(1, len(layer_l_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                l_modules.append(torch.nn.Linear(layer_l_sizes[i-1], layer_l_sizes[i]))\n",
    "                # l_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                l_modules.append(torch.nn.Linear(layer_l_sizes[i-1], layer_l_sizes[i]))\n",
    "\n",
    "        for i in range(1, len(layer_z_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                z_modules.append(torch.nn.Linear(layer_z_sizes[i-1], layer_z_sizes[i]))\n",
    "                # z_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                z_modules.append(torch.nn.Linear(layer_z_sizes[i-1], layer_z_sizes[i]))\n",
    "\n",
    "        for i in range(1, len(layer_mu_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                mu_modules.append(torch.nn.Linear(layer_mu_sizes[i-1], layer_mu_sizes[i]))\n",
    "                # mu_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                mu_modules.append(torch.nn.Linear(layer_mu_sizes[i-1], layer_mu_sizes[i]))\n",
    "        \n",
    "        self.net = torch.nn.Sequential(*shared_modules)\n",
    "        \n",
    "        self.n_features = bus_size*2\n",
    "        self.net.fc = torch.nn.Identity()\n",
    "\n",
    "        self.x_l_heads = torch.nn.ModuleList([])\n",
    "        self.z_heads = torch.nn.ModuleList([])\n",
    "        self.mu_heads = torch.nn.ModuleList([])\n",
    "        \n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_bus_modules_1\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_bus_modules_2\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_gen_modules_1\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_gen_modules_2\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *l_modules\n",
    "        ))\n",
    "        self.z_heads.append(torch.nn.Sequential(\n",
    "            *z_modules\n",
    "        ))\n",
    "        self.mu_heads.append(torch.nn.Sequential(\n",
    "            *mu_modules\n",
    "        ))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Shared_head = self.net(x)\n",
    "        output = []\n",
    "        for head in self.x_l_heads:\n",
    "            output.append(head(Shared_head))\n",
    "        for head in self.z_heads:\n",
    "            output.append(head(torch.cat((output[0], output[1], output[2], output[3]), dim=1)))\n",
    "        for head in self.mu_heads:\n",
    "            output.append(head(output[5]))\n",
    "        return output\n",
    "\n",
    "def create_model_MTL_solo(bus_size, gen_size, z_mu_size, shouldPrint, batch_size=32):\n",
    "    model = TestMultiTaskNetSolo(bus_size, gen_size, z_mu_size).to(device)\n",
    "    if (shouldPrint):\n",
    "        summary(model, (batch_size,bus_size*2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d496b-6c0e-49dd-9386-c3e2a07492cd",
   "metadata": {},
   "source": [
    "## Neuromancer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7c7f37d7-bacb-49da-a16a-c243e498bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMultiTaskNetNeuromancer(torch.nn.Module):\n",
    "    def __init__(self, bus_size, gen_size, z_mu_size):\n",
    "        super(TestMultiTaskNetV2, self).__init__()\n",
    "        # Sahred layers (Input: 28, Output: 58)\n",
    "\n",
    "        layer_1_sizes, layer_x_sizes_1, layer_x_sizes_2, layer_l_sizes, layer_z_sizes, layer_mu_sizes = generate_layers(bus_size, gen_size, z_mu_size)\n",
    "\n",
    "        print(layer_1_sizes)\n",
    "        print(layer_x_sizes_1)\n",
    "        print(layer_x_sizes_2)\n",
    "        print(layer_l_sizes)\n",
    "        print(layer_z_sizes)\n",
    "        print(layer_mu_sizes)\n",
    "        \n",
    "        shared_modules = []\n",
    "        x_bus_modules_1 = []\n",
    "        x_bus_modules_2 = []\n",
    "        x_gen_modules_1 = []\n",
    "        x_gen_modules_2 = []\n",
    "        l_modules = []\n",
    "        z_modules = []\n",
    "        mu_modules = []\n",
    "\n",
    "        for i in range(1, len(layer_1_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                shared_modules.append(torch.nn.Linear(layer_1_sizes[i-1], layer_1_sizes[i]))\n",
    "                # shared_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                shared_modules.append(torch.nn.Linear(layer_1_sizes[i-1], layer_1_sizes[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_1)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_bus_modules_1.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "                # x_bus_modules_1.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_bus_modules_1.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_1)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_bus_modules_2.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "                # x_bus_modules_2.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_bus_modules_2.append(torch.nn.Linear(layer_x_sizes_1[i-1], layer_x_sizes_1[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_2)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_gen_modules_1.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "                # x_gen_modules_1.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_gen_modules_1.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "\n",
    "        for i in range(1, len(layer_x_sizes_2)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                x_gen_modules_2.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "                # x_gen_modules_2.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                x_gen_modules_2.append(torch.nn.Linear(layer_x_sizes_2[i-1], layer_x_sizes_2[i]))\n",
    "\n",
    "        for i in range(1, len(layer_l_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                l_modules.append(torch.nn.Linear(layer_l_sizes[i-1], layer_l_sizes[i]))\n",
    "                # l_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                l_modules.append(torch.nn.Linear(layer_l_sizes[i-1], layer_l_sizes[i]))\n",
    "\n",
    "        for i in range(1, len(layer_z_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                z_modules.append(torch.nn.Linear(layer_z_sizes[i-1], layer_z_sizes[i]))\n",
    "                # z_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                z_modules.append(torch.nn.Linear(layer_z_sizes[i-1], layer_z_sizes[i]))\n",
    "\n",
    "        for i in range(1, len(layer_mu_sizes)):\n",
    "            if i != len(layer_1_sizes)-1:\n",
    "                mu_modules.append(torch.nn.Linear(layer_mu_sizes[i-1], layer_mu_sizes[i]))\n",
    "                # mu_modules.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                mu_modules.append(torch.nn.Linear(layer_mu_sizes[i-1], layer_mu_sizes[i]))\n",
    "        \n",
    "        self.net = torch.nn.Sequential(*shared_modules)\n",
    "        \n",
    "        self.n_features = bus_size*2\n",
    "        self.net.fc = torch.nn.Identity()\n",
    "\n",
    "        self.x_l_heads = torch.nn.ModuleList([])\n",
    "        self.z_heads = torch.nn.ModuleList([])\n",
    "        self.mu_heads = torch.nn.ModuleList([])\n",
    "        \n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_bus_modules_1\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_bus_modules_2\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_gen_modules_1\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *x_gen_modules_2\n",
    "        ))\n",
    "        self.x_l_heads.append(torch.nn.Sequential(\n",
    "            *l_modules\n",
    "        ))\n",
    "        self.z_heads.append(torch.nn.Sequential(\n",
    "            *z_modules\n",
    "        ))\n",
    "        self.mu_heads.append(torch.nn.Sequential(\n",
    "            *mu_modules\n",
    "        ))\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=1).to(device)\n",
    "        Shared_head = self.net(x)\n",
    "        output = []\n",
    "        for head in self.x_l_heads:\n",
    "            output.append(head(Shared_head))\n",
    "        for head in self.z_heads:\n",
    "            output.append(head(torch.cat((output[0], output[1], output[2], output[3]), dim=1)))\n",
    "        for head in self.mu_heads:\n",
    "            output.append(head(output[5]))\n",
    "        # return output\n",
    "        return [x.to(\"cpu\") for x in output]\n",
    "\n",
    "def create_model_MTL_neuromancer(bus_size, gen_size, z_mu_size, shouldPrint, batch_size=32):\n",
    "    model = TestMultiTaskNetNeuromancer(bus_size, gen_size, z_mu_size).to(device)\n",
    "    if (shouldPrint):\n",
    "        summary(model, (batch_size,bus_size*2))\n",
    "    return model\n",
    "\n",
    "def create_neuromancer_problem(func, ):\n",
    "    sol_map = Node(func, [\"Pd\", \"Qd\"], [\"x\"], name='map')\n",
    "\n",
    "    Pd = variable(\"Pd\")\n",
    "    Qd = variable(\"Qd\")\n",
    "    Va = variable(\"Va\")\n",
    "    Vm = variable(\"Vm\")\n",
    "    Pg = variable(\"Pg\")\n",
    "    Qg = variable(\"Qg\")\n",
    "    Lambda = variable(\"Lambda\")\n",
    "    Z = variable(\"Z\")\n",
    "    Mu = variable(\"Mu\")\n",
    "    Va_i = variable(\"x\")[0]\n",
    "    Vm_i = variable(\"x\")[1]\n",
    "    Pg_i = variable(\"x\")[2]\n",
    "    Qg_i = variable(\"x\")[3]\n",
    "    Lambda_i = variable(\"x\")[4]\n",
    "    Z_i = variable(\"x\")[5]\n",
    "    Mu_i = variable(\"x\")[6]\n",
    "    \n",
    "    # Objective function\n",
    "    f1 = torch.abs(Vm-Vm_i) + torch.abs(Va-Va_i)\n",
    "    f2 = torch.abs(Pg-Pg_i) + torch.abs(Qg-Qg_i)\n",
    "    f3 = torch.abs(Lambda - Lambda_i)\n",
    "    f4 = torch.abs(Z - Z_i)\n",
    "    f5 = torch.abs(Mu - Mu_i)\n",
    "    obj1 = f1.minimize(weight=0.20, name='obj1')\n",
    "    obj2 = f2.minimize(weight=0.20, name='obj2')\n",
    "    obj3 = f3.minimize(weight=0.10, name='obj3')\n",
    "    obj4 = f4.minimize(weight=0.10, name='obj4')\n",
    "    obj5 = f5.minimize(weight=0.10, name='obj5')\n",
    "    obj6 = f6.minimize(weight=0.30, name='obj6')\n",
    "    \n",
    "    # Constraints\n",
    "    Q_con = 100.  # constraint penalty weights\n",
    "    con_1 = Q_con*(V_min <= Vm_i <= V_max)\n",
    "    con_1.name = 'c1'\n",
    "    \n",
    "    constraints = [con_1]\n",
    "    for index, (min, max) in enumerate(zip(Q_min, Q_max)):\n",
    "        con = Q_con*(min <= Qg_i[:, [index]] <= max)\n",
    "        con.name = 'c2_'+str(index)\n",
    "        constraints.append(con)\n",
    "    \n",
    "    for index, max in enumerate(P_max):\n",
    "        con = Q_con*(0 <= Pg_i[:, [index]] <= max)\n",
    "        con.name = 'c3_'+str(index)\n",
    "        constraints.append(con)\n",
    "    \n",
    "    \n",
    "    # constrained optimization problem construction\n",
    "    objectives = [obj1, obj2, obj3, obj4, obj5]\n",
    "    components = [sol_map]\n",
    "    \n",
    "    # create penalty method loss function\n",
    "    loss = PenaltyLoss(objectives, constraints)\n",
    "    # construct constrained optimization problem\n",
    "    return Problem(components, loss)\n",
    "\n",
    "def prepare_trainer(problem, train_data, dev_data, test_data):\n",
    "    lr = 0.0001      # step size for gradient descent\n",
    "    epochs = 100    # number of training epochs\n",
    "    warmup = 25    # number of epochs to wait before enacting early stopping policy\n",
    "    patience = 25  # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "\n",
    "    optimizer = torch.optim.AdamW(problem.parameters(), lr=lr)\n",
    "    \n",
    "    # define trainer\n",
    "    return Trainer(\n",
    "        problem,\n",
    "        train_data,\n",
    "        dev_data,\n",
    "        test_data,\n",
    "        optimizer,\n",
    "        epochs=epochs,\n",
    "        patience=patience,\n",
    "        warmup=warmup)\n",
    "\n",
    "def train_neuromancer_model(trainer):\n",
    "    best_model = trainer.train()\n",
    "    best_outputs = trainer.test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d179d2c-124e-4af3-b27e-c15ed6351e42",
   "metadata": {},
   "source": [
    "## Main Pipeline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e41e638f-a6fa-47cb-8f15-a4e440088ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline(case_file, \n",
    "                  total_number_of_cases=10000, \n",
    "                  delta=0.1, \n",
    "                  test_percent=0.2, \n",
    "                  activations=[None, None, None, None, None],\n",
    "                  dropouts=[(False, 0),(False, 0),(False, 0),(False, 0),(False, 0)],\n",
    "                  engine=\"matlab\"):\n",
    "    fileStr = get_case_as_string(case_file)\n",
    "    bus = (get_bus_count(fileStr), get_gen_count(fileStr))\n",
    "    create_test_cases(fileStr, total_number_of_cases, delta)\n",
    "    # # label_cases(engine)\n",
    "    bus_dir = f\"generated_cases/\"\n",
    "    MTL_solo_train = None\n",
    "    MTL_solo_val = None\n",
    "    neuromancer_train_loader = None\n",
    "    neuromancer_dev_loader = None\n",
    "    neuromancer_test_loader = None\n",
    "    output_size = None\n",
    "    MTL_solo_train, MTL_solo_val = get_MTL_solo_data(bus[0], bus[1])\n",
    "    neuromancer_train_loader, neuromancer_dev_loader, neuromancer_test_loader, output_size = get_neuromancer_data(bus[0], bus[1])\n",
    "    \n",
    "    # if (not os.path.exists(bus_dir + \"neuromancerTrainData.pth\")):\n",
    "    #     train_data, dev_data, test_data, out_size = create_data(\n",
    "    #         bus_dir, bus[0] , bus[1], 1-test_percent, test_percent/2, test_percent/2)\n",
    "    #     torch.save(train_data, bus_dir + \"neuromancerTrainData.pth\")\n",
    "    #     torch.save(dev_data, bus_dir + \"neuromancerDevData.pth\")\n",
    "    #     torch.save(test_data, bus_dir + \"neuromancerTestData.pth\")\n",
    "    #     df = pd.DataFrame(columns=['output_size'])\n",
    "    #     df.loc[len(df.index)] = [out_size]\n",
    "    #     df.to_csv(bus_dir + \"output_size.csv\", header=False, index=False)\n",
    "    # else:\n",
    "    #     train_data, dev_data, test_data, out_size = load_data(bus_dir)\n",
    "    \n",
    "    # func = create_model_v2(bus[0], bus[1], load_z_mu_size(f\"bus_{str(bus[0])}_data/\"), False)\n",
    "\n",
    "    # sol_map = Node(func, [\"Pd\", \"Qd\"], [\"x\"], name='map')\n",
    "    \n",
    "    # optimizer = torch.optim.AdamW(problem.parameters(), lr=lr)\n",
    "    # problem = create_neuromancer_problem(get_constraint_constants(case_file))\n",
    "    # lr = 0.0001      # step size for gradient descent\n",
    "    # epochs = 100    # number of training epochs\n",
    "    # warmup = 25    # number of epochs to wait before enacting early stopping policy\n",
    "    # patience = 25  # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "    # # define trainer\n",
    "    # bus_118_trainer = Trainer(\n",
    "    #     problem, train_data, dev_data, test_data, optimizer,\n",
    "    #     epochs=epochs, patience=patience, warmup=warmup)\n",
    "    # # Train NLP solution map\n",
    "    # bus_118_best_model = bus_118_trainer.train()\n",
    "    # best_outputs = bus_118_trainer.test(bus_118_best_model)\n",
    "    # # load best model dict\n",
    "    # problem.load_state_dict(bus_118_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7aa08ee3-2130-463f-bcd2-2d2e3e4f01a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases already generated, skipping...\n",
      "Data Loaders found, loading from .pth files\n",
      "\n",
      "7000\n",
      "8500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [00:19<00:00, 517.95it/s]\n"
     ]
    }
   ],
   "source": [
    "full_pipeline(\"case14.m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b65e7f-867a-4337-9e1d-371bc36c241a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
